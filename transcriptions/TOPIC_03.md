# LLMs and Generative AI

Bem-vindo a esta seção do curso, onde vamos explorar os elementos-chave para alcançar sucesso com IA generativa. Começaremos examinando as diversas aplicações de LLMs (modelos de linguagem de grande escala). Em seguida, exploraremos os recursos oferecidos pelo Databricks especificamente para IA generativa. Por fim, apresentaremos um modelo para adotar efetivamente a IA generativa em sua organização. Quando o GPT-3.5 foi lançado, acreditava-se que seria difícil replicar seus resultados, mas em 12 meses, muitos modelos proprietários começaram a competir. Agora, os preços estão caindo dez vezes por ano. Mais impressionante ainda, os modelos de código aberto estão melhorando imensamente, com a comunidade de pesquisa colaborando e compartilhando técnicas. Esses modelos de código aberto estão rapidamente alcançando uma qualidade aceitável para modelos de base em muitos casos empresariais. No entanto, a má notícia é que, se você tem acesso a isso, seus concorrentes também têm. Dominar as técnicas de modelagem é uma condição necessária para competir, mas não suficiente para se diferenciar no mercado. A principal forma de se destacar é aproveitar seus dados. Seus dados o diferenciam da concorrência. Use os dados exclusivos da sua organização para construir grandes aplicações e desbloqueie enormes quantidades de dados proprietários que estão parados.

Se você está considerando construir uma aplicação usando LLMs, uma das primeiras perguntas pode ser: qual modelo devo usar? Existem duas opções principais, e vamos discuti-las. A primeira opção é usar modelos de código aberto. Esses modelos podem ser usados como estão ou ajustados conforme suas necessidades, oferecendo flexibilidade para personalização e geralmente sendo menores em tamanho, o que reduz custos. A segunda opção é usar modelos proprietários. Esses modelos são treinados em grandes conjuntos de dados e geralmente oferecidos como soluções de LLM como serviço. No entanto, as licenças para modelos proprietários frequentemente têm restrições de uso e modificação. Cada opção tem suas próprias considerações, e a escolha depende de fatores como orçamento, requisitos de personalização e restrições de licenciamento.

Ao escolher o modelo certo para suas necessidades, é importante entender que não existe um modelo perfeito, e cada tipo tem seus trade-offs. Aqui estão alguns fatores a considerar na sua decisão: Privacidade – como seus dados serão usados? É essencial garantir a segurança e a proteção dos seus dados, considerando aspectos como manuseio, armazenamento e a possibilidade de exclusão, se necessário. Qualidade – entender como o modelo foi treinado é crucial. Avalie a precisão e a confiabilidade das previsões do modelo, buscando informações sobre o conjunto de dados usado no treinamento e possíveis vieses que possam afetar seu desempenho. Custo – determine seu orçamento para usar o modelo, considerando o custo de aquisição, os requisitos de infraestrutura adicional e as despesas de manutenção contínua. Latência – avalie o tempo de processamento necessário pelo modelo. Verifique se o tempo de resposta do modelo atende às necessidades do seu negócio, especialmente em aplicações em tempo real ou sensíveis ao tempo. Ao considerar cuidadosamente esses fatores, você pode tomar uma decisão informada sobre qual modelo melhor se alinha aos seus requisitos de privacidade, expectativas de qualidade, restrições orçamentárias e desejos de latência.

Como discutido anteriormente, uma das opções para usar LLMs é os modelos proprietários. Vamos explorar as vantagens e limitações desses tipos de modelos. Vamos nos aprofundar no uso de modelos proprietários ou LLMs como serviço. Aqui estão os prós: Velocidade de desenvolvimento – como os modelos proprietários já existem, é rápido começar a usá-los; essencialmente, é uma chamada de API que se integra bem aos seus pipelines existentes. Desempenho – como o processamento é feito no lado do servidor, você pode usar modelos maiores com melhor desempenho e qualidade. Elementos proprietários frequentemente entregam saídas de alta qualidade, pois são treinados em conjuntos de dados extensos e especializados, resultando em previsões mais precisas e adaptadas para casos de uso específicos.

Agora, vamos olhar os contras: Custo – como você paga por cada solicitação enviada ou recebida por um fornecedor, os custos podem ser bastante altos. Privacidade e segurança de dados – você sabe como seus dados estão sendo usados? Você confia no fornecedor para agir no melhor interesse dos seus dados? Você trabalha com dados confidenciais, onde um vazamento seria catastrófico para o negócio? Bloqueio do fornecedor – o bloqueio por fornecedor pode ser uma desvantagem potencial de usar LLMs proprietários, tornando as empresas suscetíveis a quedas do fornecedor, recursos obsoletos e outras limitações que podem surgir ao depender exclusivamente das ofertas de um fornecedor específico.

Quando se trata de usar modelos de código aberto, há vários prós a considerar. Você tem a flexibilidade de personalizar os modelos para atender às suas tarefas específicas, permitindo alcançar os resultados desejados. Além disso, como esses modelos podem ser continuados e ajustados, tendem a ser menores em tamanho, resultando em possíveis economias de custo. Além disso, você mantém controle total sobre seus dados e informações do modelo. No entanto, também há alguns contras a serem observados. Os custos associados ao treinamento e à computação podem ser significativos. Além disso, modelos maiores podem exigir conjuntos de dados maiores, e cabe a você criá-los, curá-los, comprá-los ou obtê-los. Há também preocupações sobre a qualidade dos modelos e a necessidade de expertise interna para configurá-los e implantá-los adequadamente.

Um conceito importante a considerar em sua jornada com LLMs é se você deseja pré-treinar um modelo. Pré-treinar um modelo é essencialmente como ensinar ao modelo as regras gerais básicas de uma linguagem. Se pensarmos nisso em termos de aprender uma língua, pré-treinar um modelo seria como você ler livros, artigos, transcrições de seus filmes favoritos, blogs, etc., nessa língua. Ao fazer isso, você pode aprendê-la, falar com certa facilidade, e até se familiarizar com a sintaxe da língua, talvez com gírias ou o fluxo geral de como as palavras são ordenadas. Mas, se lhe pedissem para falar sobre um tópico muito específico, você pode não ter o conhecimento ou a expertise de domínio para fazer isso nessa nova língua. Voltando ao mundo dos LLMs, digamos que você tenha um caso de uso legal muito específico – por exemplo, em sua organização, você quer um modelo jurídico para coisas como geração de memoriais legais ou perguntas e respostas jurídicas. Você pode não ter o direito de usar todos os dados com os quais esse modelo foi treinado. Nesse caso, você pode optar por pré-treinar um modelo de base. Em outras palavras, em vez de usar um modelo de código aberto treinado com todas as informações que encontra na web ou dados aos quais você pode não ter acesso ou direito legal de usar, você pode criar um modelo de domínio específico mais eficiente a partir do zero, usando seus próprios dados ou dados que você curou. Provavelmente será um modelo menor – pelo menos é geralmente o caso –, mas não só pode alcançar a mesma qualidade de um modelo de código aberto, como também pode superá-la, reduzindo o que é conhecido como alucinação. Alucinação é um fenômeno em que o modelo pode gerar saídas que parecem plausíveis, mas são imprecisas ou sem sentido devido a limitações em seu entendimento. Se você decidir pré-treinar um modelo de base, também pode ajustá-lo, o que discutiremos no próximo slide.

Ao usar modelos de código aberto, você tem duas opções: usá-los como estão ou ajustá-los com base em seu caso de uso específico. Da mesma forma, alguns modelos proprietários também permitem ajustes. Ajustar um modelo é um conceito crucial na IA generativa, então vamos explorar sua definição e como é aplicado em casos de uso específicos. Ajustar um modelo envolve pegar um modelo já treinado e treiná-lo ainda mais para realizar uma tarefa específica ou adaptá-lo para uma aplicação ou domínio particular. Normalmente, um modelo de base é inicialmente treinado em um grande conjunto de dados; depois, você pega esse modelo de base e o treina em um conjunto de dados menor, permitindo que melhore suas capacidades preditivas com base em seus casos de uso específicos. Por exemplo, se você deseja ajustar modelos para tarefas específicas, como resposta a perguntas, análise de sentimentos ou reconhecimento de entidades nomeadas, você começa com um modelo de base e realiza treinamento supervisionado usando conjuntos de dados menores e rotulados. Esse processo treina o modelo para executar suas tarefas desejadas. Por exemplo, se você precisa que o modelo responda a perguntas, forneça pares de perguntas e respostas para treiná-lo. Se estiver focado em análise de sentimentos, pode usar mensagens de texto ou avaliações de clientes para treinar o modelo. Alternativamente, você pode rotular nomes de pessoas ou locais em um conjunto de dados para treinar o modelo para reconhecimento de entidades nomeadas.

Digamos que você esteja ajustando modelos para adaptação de domínio, focando em áreas como ciência, finanças e domínios jurídicos. Para isso, você usaria revistas científicas, documentos financeiros ou documentos jurídicos para adaptar o modelo especificamente aos seus casos de uso. Esse processo ajuda os modelos a aprender conhecimento específico do domínio e aprimorar seu desempenho nos respectivos campos. Como você provavelmente sabe, os modelos de código aberto estão melhorando, e a inovação está avançando rapidamente. Dolly foi o primeiro modelo de código aberto que segue instruções e está disponível para uso comercial, lançado pelo Databricks em 2023. Desde então, outros modelos foram lançados, incluindo o modelo Mixtral 8x7B, que ganhou destaque em meados de 2023. Desde 2023, ainda mais modelos foram liberados, como o DBRX, um LLM de código aberto lançado pelo Databricks em março de 2024. Na época do anúncio, ele estabeleceu novos benchmarks na indústria para compreensão de linguagem, programação, matemática e lógica. O mais interessante sobre o DBRX é que ele usa a plataforma de inteligência de dados do Databricks para construir LLMs. Agora, construir um LLM personalizado é enriquecido por cada empresa que usa o Databricks; você não precisa mais ser um provedor especializado de modelos para construir um LLM.

Até agora, comparamos LLMs de código aberto e proprietários. No entanto, em muitas aplicações baseadas em LLMs, várias tarefas estão envolvidas, e pode ser necessário usar múltiplos LLMs no mesmo fluxo de trabalho. Neste exemplo de fluxo de trabalho, enquanto um LLM de código aberto pode ser usado para tarefas como sumarização e análise de sentimentos, um LLM SaaS pode ser mais adequado para geração de conteúdo. Para facilitar essa abordagem, a indústria introduziu o conceito de *chain*. Uma ferramenta como LangChain permite a integração perfeita dessas chamadas de modelo. Além disso, é importante usar um banco de dados vetorial para armazenar o estado da *chain*. Um banco de dados vetorial oferece armazenamento e recuperação eficientes das representações intermediárias geradas durante o processo de encadeamento, facilitando a gestão de fluxo de trabalho e a coordenação de tarefas.

Aqui está um exemplo de dividir um fluxo de trabalho em várias tarefas e usar LLMs para cada uma. Na solução inicial, todos os artigos são analisados juntos, o que pode levar a desafios devido a limitações de entrada do modelo e aos custos associados a entradas longas. Uma solução mais eficaz, como mostrado no segundo diagrama, é empregar um LLM para sumarização de artigos. O texto resumido pode então servir como entrada para o LLM de análise de sentimentos, possibilitando um processamento mais eficiente e reduzindo problemas potenciais causados por restrições de comprimento de entrada. Existe também o conceito conhecido como geração aumentada por recuperação, ou RAG, que é uma aplicação popular de LLM que permite criar um sistema onde seu modelo pode acessar fontes de dados externas para completar sua tarefa. O que isso parece na prática? Vamos analisar o exemplo nesta apresentação. Isso mostra o retraining ou ajuste de um modelo. Neste exemplo, temos nosso modelo original treinado com os dados originais com os quais foi treinado. O que acontece quando obtemos uma nova fonte de dados ou temos dados atualizados é que frequentemente precisamos ajustar ou retrenar nosso modelo para que ele continue gerando saídas confiáveis. Com a geração aumentada por recuperação, ou RAG, você pode criar um sistema de busca. Quando você faz uma pergunta, o sistema de busca procura fontes ou dados relevantes que você curou em um banco de dados vetorial. Quando retorna essas informações relevantes, elas são combinadas com sua pergunta original e alimentam o modelo. Por que isso é tão poderoso? Porque significa que seu modelo agora tem os dados mais recentes que você possui, o que melhora o desempenho do modelo. Ele pode usar esses dados, além dos dados com os quais já foi treinado, para gerar sua saída. Isso significa que, primeiro, você pode gastar menos tempo retrainando seu modelo e, segundo, as fontes relevantes podem ajudar a reduzir alucinações.
