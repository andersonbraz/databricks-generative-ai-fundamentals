# Aplicações de LLM

Bem-vindo a esta seção do curso, onde vamos explorar os elementos-chave para alcançar sucesso com IA generativa. Começaremos examinando as diversas aplicações de LLMs (modelos de linguagem de grande escala). Em seguida, exploraremos os recursos oferecidos pelo Databricks especificamente para IA generativa. Por fim, apresentaremos um modelo para adotar efetivamente a IA generativa em sua organização. Quando o GPT-3.5 foi lançado, acreditava-se que seria difícil replicar seus resultados, mas em 12 meses, muitos modelos proprietários começaram a competir. Agora, os preços estão caindo dez vezes por ano. Mais impressionante ainda, os modelos de código aberto estão melhorando imensamente, com a comunidade de pesquisa colaborando e compartilhando técnicas. Esses modelos de código aberto estão rapidamente alcançando uma qualidade aceitável para modelos de base em muitos casos empresariais. No entanto, a má notícia é que, se você tem acesso a isso, seus concorrentes também têm. Dominar as técnicas de modelagem é uma condição necessária para competir, mas não suficiente para se diferenciar no mercado. A principal forma de se destacar é aproveitar seus dados. Seus dados o diferenciam da concorrência. Use os dados exclusivos da sua organização para construir grandes aplicações e desbloqueie enormes quantidades de dados proprietários que estão parados.

## Usando LLMs

Se você está considerando construir uma aplicação usando LLMs, uma das primeiras perguntas pode ser: qual modelo devo usar? Existem duas opções principais, e vamos discuti-las. A primeira opção é usar modelos de código aberto. Esses modelos podem ser usados como estão ou ajustados conforme suas necessidades, oferecendo flexibilidade para personalização e geralmente sendo menores em tamanho, o que reduz custos. A segunda opção é usar modelos proprietários. Esses modelos são treinados em grandes conjuntos de dados e geralmente oferecidos como soluções de LLM como serviço. No entanto, as licenças para modelos proprietários frequentemente têm restrições de uso e modificação. Cada opção tem suas próprias considerações, e a escolha depende de fatores como orçamento, requisitos de personalização e restrições de licenciamento.

Ao escolher o modelo certo para suas necessidades, é importante entender que não existe um modelo perfeito, e cada tipo tem seus trade-offs. Aqui estão alguns fatores a considerar na sua decisão: 

**Privacidade** – como seus dados serão usados? É essencial garantir a segurança e a proteção dos seus dados, considerando aspectos como manuseio, armazenamento e a possibilidade de exclusão, se necessário.

**Qualidade** – entender como o modelo foi treinado é crucial. Avalie a precisão e a confiabilidade das previsões do modelo, buscando informações sobre o conjunto de dados usado no treinamento e possíveis vieses que possam afetar seu desempenho. 

**Custo** – determine seu orçamento para usar o modelo, considerando o custo de aquisição, os requisitos de infraestrutura adicional e as despesas de manutenção contínua. 

**Latência** – avalie o tempo de processamento necessário pelo modelo. Verifique se o tempo de resposta do modelo atende às necessidades do seu negócio, especialmente em aplicações em tempo real ou sensíveis ao tempo. Ao considerar cuidadosamente esses fatores, você pode tomar uma decisão informada sobre qual modelo melhor se alinha aos seus requisitos de privacidade, expectativas de qualidade, restrições orçamentárias e desejos de latência.

Como discutido anteriormente, uma das opções para usar LLMs é os modelos proprietários. Vamos explorar as vantagens e limitações desses tipos de modelos. Vamos nos aprofundar no uso de modelos proprietários ou LLMs como serviço. Aqui estão os prós: **Velocidade de desenvolvimento** – como os modelos proprietários já existem, é rápido começar a usá-los; essencialmente, é uma chamada de API que se integra bem aos seus pipelines existentes. **Desempenho** – como o processamento é feito no lado do servidor, você pode usar modelos maiores com melhor desempenho e qualidade. Elementos proprietários frequentemente entregam saídas de alta qualidade, pois são treinados em conjuntos de dados extensos e especializados, resultando em previsões mais precisas e adaptadas para casos de uso específicos.

### Os contras

Agora, vamos olhar os contras: **Custo** – como você paga por cada solicitação enviada ou recebida por um fornecedor, os custos podem ser bastante altos. **Privacidade e segurança de dados** – você sabe como seus dados estão sendo usados? Você confia no fornecedor para agir no melhor interesse dos seus dados? Você trabalha com dados confidenciais, onde um vazamento seria catastrófico para o negócio? **Bloqueio do fornecedor** – o bloqueio por fornecedor pode ser uma desvantagem potencial de usar LLMs proprietários, tornando as empresas suscetíveis a quedas do fornecedor, recursos obsoletos e outras limitações que podem surgir ao depender exclusivamente das ofertas de um fornecedor específico.

### Os prós

Quando se trata de usar modelos de código aberto, há vários prós a considerar. Você tem a **flexibilidade de personalizar os modelos** para atender às suas tarefas específicas, permitindo alcançar os resultados desejados. Além disso, como esses **modelos podem ser continuados e ajustados**, tendem a ser menores em tamanho, resultando em possíveis economias de custo. Além disso, você mantém controle total sobre seus dados e informações do modelo. No entanto, também há alguns contras a serem observados. Os custos associados ao treinamento e à computação podem ser significativos. Além disso, modelos maiores podem exigir conjuntos de dados maiores, e cabe a você criá-los, curá-los, comprá-los ou obtê-los. Há também preocupações sobre a qualidade dos modelos e a necessidade de expertise interna para configurá-los e implantá-los adequadamente.

Um conceito importante a considerar em sua jornada com LLMs é se você deseja pré-treinar um modelo. **Pré-treinar um modelo é essencialmente como ensinar ao modelo as regras gerais básicas de uma linguagem**. Se pensarmos nisso em termos de aprender uma língua, pré-treinar um modelo seria como você ler livros, artigos, transcrições de seus filmes favoritos, blogs, etc., nessa língua. Ao fazer isso, você pode aprendê-la, falar com certa facilidade, e até se familiarizar com a sintaxe da língua, talvez com gírias ou o fluxo geral de como as palavras são ordenadas. Mas, se lhe pedissem para falar sobre um tópico muito específico, você pode não ter o conhecimento ou a expertise de domínio para fazer isso nessa nova língua. Voltando ao mundo dos LLMs, digamos que você tenha um caso de uso legal muito específico – por exemplo, em sua organização, você quer um modelo jurídico para coisas como geração de memoriais legais ou perguntas e respostas jurídicas. Você pode não ter o direito de usar todos os dados com os quais esse modelo foi treinado. Nesse caso, você pode optar por pré-treinar um modelo de base. Em outras palavras, em vez de usar um modelo de código aberto treinado com todas as informações que encontra na web ou dados aos quais você pode não ter acesso ou direito legal de usar, você pode criar um modelo de domínio específico mais eficiente a partir do zero, usando seus próprios dados ou dados que você curou. Provavelmente será um modelo menor – pelo menos é geralmente o caso –, mas não só pode alcançar a mesma qualidade de um modelo de código aberto, como também pode superá-la, reduzindo o que é conhecido como alucinação. Alucinação é um fenômeno em que o modelo pode gerar saídas que parecem plausíveis, mas são imprecisas ou sem sentido devido a limitações em seu entendimento. Se você decidir pré-treinar um modelo de base, também pode ajustá-lo, o que discutiremos no próximo slide.

Ao usar modelos de código aberto, você tem duas opções: usá-los como estão ou ajustá-los com base em seu caso de uso específico. Da mesma forma, alguns modelos proprietários também permitem ajustes. Ajustar um modelo é um conceito crucial na IA generativa, então vamos explorar sua definição e como é aplicado em casos de uso específicos. Ajustar um modelo envolve pegar um modelo já treinado e treiná-lo ainda mais para realizar uma tarefa específica ou adaptá-lo para uma aplicação ou domínio particular. Normalmente, um modelo de base é inicialmente treinado em um grande conjunto de dados; depois, você pega esse modelo de base e o treina em um conjunto de dados menor, permitindo que melhore suas capacidades preditivas com base em seus casos de uso específicos. Por exemplo, se você deseja ajustar modelos para tarefas específicas, como resposta a perguntas, análise de sentimentos ou reconhecimento de entidades nomeadas, você começa com um modelo de base e realiza treinamento supervisionado usando conjuntos de dados menores e rotulados. Esse processo treina o modelo para executar suas tarefas desejadas. Por exemplo, se você precisa que o modelo responda a perguntas, forneça pares de perguntas e respostas para treiná-lo. Se estiver focado em análise de sentimentos, pode usar mensagens de texto ou avaliações de clientes para treinar o modelo. Alternativamente, você pode rotular nomes de pessoas ou locais em um conjunto de dados para treinar o modelo para reconhecimento de entidades nomeadas.

Digamos que você esteja ajustando modelos para adaptação de domínio, focando em áreas como ciência, finanças e domínios jurídicos. Para isso, você usaria revistas científicas, documentos financeiros ou documentos jurídicos para adaptar o modelo especificamente aos seus casos de uso. Esse processo ajuda os modelos a aprender conhecimento específico do domínio e aprimorar seu desempenho nos respectivos campos. Como você provavelmente sabe, os modelos de código aberto estão melhorando, e a inovação está avançando rapidamente. Dolly foi o primeiro modelo de código aberto que segue instruções e está disponível para uso comercial, lançado pelo Databricks em 2023. Desde então, outros modelos foram lançados, incluindo o modelo Mixtral 8x7B, que ganhou destaque em meados de 2023. Desde 2023, ainda mais modelos foram liberados, como o DBRX, um LLM de código aberto lançado pelo Databricks em março de 2024. Na época do anúncio, ele estabeleceu novos benchmarks na indústria para compreensão de linguagem, programação, matemática e lógica. O mais interessante sobre o DBRX é que ele usa a plataforma de inteligência de dados do Databricks para construir LLMs. Agora, construir um LLM personalizado é enriquecido por cada empresa que usa o Databricks; você não precisa mais ser um provedor especializado de modelos para construir um LLM.

Até agora, comparamos LLMs de código aberto e proprietários. No entanto, em muitas aplicações baseadas em LLMs, várias tarefas estão envolvidas, e pode ser necessário usar múltiplos LLMs no mesmo fluxo de trabalho. Neste exemplo de fluxo de trabalho, enquanto um LLM de código aberto pode ser usado para tarefas como sumarização e análise de sentimentos, um LLM SaaS pode ser mais adequado para geração de conteúdo. Para facilitar essa abordagem, a indústria introduziu o conceito de *chain*. Uma ferramenta como LangChain permite a integração perfeita dessas chamadas de modelo. Além disso, é importante usar um banco de dados vetorial para armazenar o estado da *chain*. Um banco de dados vetorial oferece armazenamento e recuperação eficientes das representações intermediárias geradas durante o processo de encadeamento, facilitando a gestão de fluxo de trabalho e a coordenação de tarefas.

Aqui está um exemplo de dividir um fluxo de trabalho em várias tarefas e usar LLMs para cada uma. Na solução inicial, todos os artigos são analisados juntos, o que pode levar a desafios devido a limitações de entrada do modelo e aos custos associados a entradas longas. Uma solução mais eficaz, como mostrado no segundo diagrama, é empregar um LLM para sumarização de artigos. O texto resumido pode então servir como entrada para o LLM de análise de sentimentos, possibilitando um processamento mais eficiente e reduzindo problemas potenciais causados por restrições de comprimento de entrada. Existe também o conceito conhecido como geração aumentada por recuperação, ou RAG, que é uma aplicação popular de LLM que permite criar um sistema onde seu modelo pode acessar fontes de dados externas para completar sua tarefa. O que isso parece na prática? Vamos analisar o exemplo nesta apresentação. Isso mostra o retraining ou ajuste de um modelo. Neste exemplo, temos nosso modelo original treinado com os dados originais com os quais foi treinado. O que acontece quando obtemos uma nova fonte de dados ou temos dados atualizados é que frequentemente precisamos ajustar ou retrenar nosso modelo para que ele continue gerando saídas confiáveis. Com a geração aumentada por recuperação, ou RAG, você pode criar um sistema de busca. Quando você faz uma pergunta, o sistema de busca procura fontes ou dados relevantes que você curou em um banco de dados vetorial. Quando retorna essas informações relevantes, elas são combinadas com sua pergunta original e alimentam o modelo. Por que isso é tão poderoso? Porque significa que seu modelo agora tem os dados mais recentes que você possui, o que melhora o desempenho do modelo. Ele pode usar esses dados, além dos dados com os quais já foi treinado, para gerar sua saída. Isso significa que, primeiro, você pode gastar menos tempo retrainando seu modelo e, segundo, as fontes relevantes podem ajudar a reduzir alucinações.

## IA do Databricks

Bem-vindo a esta seção do curso, onde vamos explorar os elementos-chave para alcançar sucesso com o Databricks AI. Antes de começarmos, aqui vai uma nota rápida: o conteúdo deste vídeo evoluirá rapidamente. Não se surpreenda se você assistir a esta aula uma semana e, na próxima, ela já estiver atualizada. Estamos antecipando isso, pois temos cada vez mais informações para compartilhar. Então, vamos começar!

Ao longo deste módulo, falaremos mais sobre como a plataforma de inteligência de dados do Databricks, ou EDI, capacita você a obter valor comercial a partir de casos de uso de IA. Também discutiremos por que desenvolver e utilizar IA generativa no Databricks é vantajoso. Se você já leu um de nossos posts no blog, fez um curso na Databricks Academy ou revisou páginas de documentação da Databricks, provavelmente já se deparou com uma imagem semelhante à mostrada neste slide. Hoje, vamos focar na área no canto superior esquerdo dessa imagem: o Databricks AI.

O Databricks AI oferece uma solução abrangente para as necessidades de IA. Ele permite criar modelos personalizados do zero ou ajustar modelos existentes, tudo isso garantindo privacidade e controle dos dados. Nosso recurso de servidão de modelos agiliza o processo de transição dos modelos para a produção, tornando-o altamente eficiente. Nossas APIs de integração, como a Open AI Direct, por exemplo, permitem construir agentes conversacionais de alta qualidade que agregam enorme valor aos seus clientes. Mas as capacidades do Databricks vão além da construção e servição de modelos. Apoiamos o fluxo de trabalho completo de IA, ajudando você a implantar e gerenciar seus modelos ao longo de seu ciclo de vida em produção.

Nosso recurso de *tuning* transparente auxilia na identificação rápida de modelos ideais sem comprometer seu controle sobre o processo. Também oferecemos monitoramento de modelos para fornecer insights sobre seu desempenho ao longo do tempo e recursos de governança para garantir reprodutibilidade e conformidade com padrões regulatórios. O melhor de tudo é que isso é construído sobre a base aberta do MLflow, assegurando integração perfeita com o ecossistema de IA mais amplo.

O paradigma do Databricks AI continua sendo o que temos há anos: você coleta conjuntos de dados, limpa esses conjuntos, realiza ETL, constrói modelos com base nesses dados ou personaliza modelos existentes – essa é a parte que muda um pouco, e vamos detalhar isso em breve. Depois, você serve esses modelos e os integra em aplicações. Vindo do lado dos modelos, treinar modelos muito grandes pode custar milhões de dólares apenas em termos computacionais, como tempo na nuvem. Em vez de treinar algo do zero, existem métodos para agilizar isso. Por exemplo, modelos já treinados em grandes conjuntos de dados podem ser ajustados – um processo chamado *fine-tuning* –, incorporando seus dados a um modelo já treinado.

Para todas essas etapas, a parte mais desafiadora é a relacionada aos dados: coletar grandes conjuntos de dados e integrá-los aos seus modelos. Acreditamos que você deve fazer isso dentro da sua plataforma de dados, com o mesmo modelo de controle de acesso, o mesmo modelo de dados, a mesma governança e as mesmas interfaces. Nos últimos anos, o Databricks focou em atualizar todas as partes-chave da plataforma para funcionar melhor com IA generativa. Há muitas semelhanças com a IA convencional na construção de aplicações, mas também há diferenças importantes, que mencionaremos.

No início deste módulo, destaquei que o atualizaremos sempre que possível. À medida que mais recursos forem lançados, garantiremos não apenas a atualização deste curso, mas também a oferta de cursos separados para capacitá-lo em cada um desses recursos. No Databricks, acreditamos que a IA generativa vai impactar praticamente todas as indústrias. Estes slides destacam apenas quatro principais. Vamos começar com a tecnologia – obviamente, o mundo tech já está sendo disruptivo, e vemos empresas mudando significativamente seus produtos para acompanhar. Como isso começou a mudar no setor tech? Fazemos parte da equipe da Databricks Academy, então coisas como gerar conteúdo a partir de prompts mínimos, permitindo que mais pessoas criem conteúdo, são exemplos. E que tal um LLM como médico? Imagine fornecer seus registros pessoais a um modelo seguro e confidencial, fazer perguntas e obter respostas que antes eram impossíveis ou imprecisas. Imagine se um modelo seguro tivesse todo o seu histórico à disposição para oferecer o melhor cuidado – algo que queremos que médicos humanos façam, mas que é logisticamente difícil. Com IA generativa, podemos começar a realizar isso. Até mesmo a anotação automática de exames: imagine fazer ultrassons e saber imediatamente o que parece fora do normal.

Uma área enorme sendo transformada pela IA generativa é o setor bancário e financeiro, principalmente na redução de custos operacionais, como preencher automaticamente formulários de empréstimos ou escrever documentos internos, como requisitos de empréstimos. Também estamos vendo mais investimentos em IA generativa na indústria farmacêutica. Desenvolver um medicamento exige muito dinheiro, então qualquer redução de tempo é crucial. A genômica é outra área onde a IA generativa parece funcionar bem.

Um aviso importante, que ainda não discutimos e que é relevante para todos que se enquadram em um caso de uso abordado no slide anterior ou em outros, é o problema com ofertas prontas de LLMs. Essas ofertas podem fornecer resultados interessantes – é realmente incrível ver um bot com quem você pode conversar como se fosse uma pessoa. Mas você precisa pensar até onde isso pode levá-lo em uma empresa. Na verdade, você precisa de algo muito mais direcionado. Não gostaria, por exemplo, de criar um bot de suporte que de repente começasse a falar sobre política com seus clientes, se você é um banco. Você precisa de algo que permaneça alinhado ao seu domínio e caso de uso, o que nos leva de volta à construção de soluções mais personalizadas para seu setor usando seus próprios dados.

Outra forma de pensar sobre isso é que provavelmente não existe um único modelo de aprendizado de máquina gigante que possa lidar com todos os casos de uso, assim como em sua equipe de ciência de dados provavelmente não há uma única pessoa que consiga fazer o trabalho de todos. À medida que avançamos, envisionamos um mundo com cada vez mais modelos possuídos por várias empresas. Com isso em mente, e enquanto você trabalha na incorporação de IA generativa, aqui vai um conselho: seja cuidadoso com seus dados. Mantenha-os próximos; seus dados são sua vantagem competitiva. Cuidado com modelos prontos que pedem seus dados. Em vez disso, procure uma solução que use seus dados e casos de uso para criar soluções personalizadas.

Certo, para onde o Databricks está indo com isso? Você pode ter ouvido que concluímos a aquisição da MosaicML. A MosaicML é uma plataforma líder na criação e personalização de modelos de IA generativa para empresas. Com a MosaicML, planejamos oferecer a melhor experiência de classe para treinar, personalizar e implantar aplicações de IA generativa. Estamos trabalhando junto com a equipe da MosaicML para acelerar isso. Vemos três desenvolvimentos essenciais para levar a IA generativa ao mainstream empresarial: primeiro, a democratização rápida das capacidades dos modelos, um futuro onde eles estejam amplamente disponíveis para todas as empresas; segundo, fazer com que modelos de IA generativa funcionem para empresas, tornando muito mais fácil para elas incorporar seus próprios dados e implantar IA segura, confiável e eficaz; terceiro, unificar a pilha de IA e dados, garantindo que, em cada etapa do ciclo de vida do aprendizado de máquina, seus dados e modelos sejam curados juntos para construir as melhores aplicações.

À medida que tivermos atualizações, garantiremos a comunicação com você por meio deste curso, seus representantes de conta, posts no blog e mais. Por enquanto, lembre-se destas três coisas: primeiro, modelos personalizados ajudarão você a aproveitar ao máximo seus dados e casos de uso; segundo, tudo isso precisa ser mantido em um ambiente seguro; terceiro, seus dados são sua vantagem competitiva para explorar.

## Preparação para adoção da IA


