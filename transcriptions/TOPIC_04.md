# Legal and Ethical Considerations

## Riscos e desafios potenciais

Nesta seção, vamos explorar os potenciais riscos associados à inteligência artificial generativa. Examinaremos considerações legais, éticas e as implicações da interação entre humanos e IA no futuro da força de trabalho e do ambiente de trabalho. Além de seus grandes benefícios potenciais, a IA generativa traz novos riscos e desafios para empresas e sociedade. Houve muitos debates sobre os riscos e benefícios da IA em geral, e você provavelmente já ouviu notícias sobre cientistas e pioneiros da IA emitindo declarações sobre os potenciais riscos dos sistemas de IA para a humanidade. Nesta seção, exploraremos os principais riscos e desafios e proporemos algumas boas práticas para mitigar esses riscos.

Questões legais, como privacidade, segurança e propriedade intelectual, estão entre as preocupações importantes. Desafios éticos surgem de vieses nos dados e algoritmos, além dos riscos associados à desinformação. O impacto da IA na sociedade e na força de trabalho também é um aspecto altamente controverso atualmente. Todos estão se perguntando a mesma coisa: a IA vai substituir meu emprego? Como mostrado neste diagrama, nem todos os riscos têm o mesmo potencial de dano. Além disso, enquanto alguns riscos são relativamente fáceis de enfrentar, outros podem representar desafios maiores para empresas e indivíduos.

Os aspectos legais e éticos da IA generativa estão evoluindo rapidamente, e muitos debates têm surgido em torno deles. Assim, é importante que você considere esses fatores no contexto do seu próprio caso.

## Considerações Legais

Vamos começar com o aspecto legal da IA generativa. O primeiro desafio que a IA generativa trouxe é a preocupação com privacidade. Os modelos atuais de IA generativa carecem de uma função de esquecimento para dados pessoais, pois são treinados em grandes quantidades de dados que podem, inadvertidamente, expor informações pessoais, potencialmente violando os direitos de privacidade dos indivíduos. Portanto, é crucial que as empresas estejam cientes desses riscos e tomem as medidas necessárias para garantir que não violem esses direitos ao construir e usar IA generativa. Aqui estão alguns pontos-chave a considerar para proteger a privacidade dos dados: use sua estratégia de dados existente como base para sua estratégia de privacidade e IA; defina os tipos de consentimento ou permissão que você pode precisar; forneça treinamento aos funcionários sobre práticas de privacidade de dados; estabeleça políticas claras da empresa sobre o uso de ferramentas de IA generativa; desenvolva um plano de violação para lidar com incidentes de privacidade.

Quando usar serviços proprietários prontos, determine que tipo de dados será coletado; entenda se seus dados serão usados para treinar modelos ou compartilhados com terceiros; certifique-se de ter capacidades de linhagem de dados que permitam excluir dados de várias partes do desenvolvimento do modelo, se necessário; avalie se o histórico de interações do usuário é armazenado e garanta a segurança. Além das considerações gerais discutidas, aqui estão algumas boas práticas para garantir a privacidade dos dados: ao processar dados para treinar modelos de IA ou usá-los como entrada, proteja seus dados, normalizando e criptografando adequadamente, e implemente controles de acesso robustos. Isso ajudará a proteger informações sensíveis durante o treinamento, armazenamento e uso. Estabeleça governança: crie um framework para governança de dados e modelos, incluindo práticas como controle de versão, monitoramento, auditoria e definição de uso de dados. Isso garante que você tenha controle sobre seus dados e modelos ao longo de seu ciclo de vida.

Além das preocupações com privacidade, os modelos de IA também apresentam desafios de segurança. Um problema notável é o vazamento de dados. A IA generativa pode memorizar e reproduzir dados de treinamento, levantando preocupações quando informações sensíveis ou confidenciais estão incluídas nos dados de treinamento ou prompts. Um exemplo ilustrativo disso ocorreu na Samsung, onde um funcionário acidentalmente compartilhou segredos da empresa com o ChatGPT, e descobriu-se que o ChatGPT reteve e memorizou essas informações. Outro risco de segurança para modelos de IA generativa é a exploração do modelo por meio de prompts, conhecida como injeção de prompts. Isso acontece quando usuários inserem instruções específicas para manipular o comportamento normal do modelo. No exemplo mostrado, o modelo inicialmente recusa-se a divulgar informações solicitadas, mas, ao modificar o prompt, fornece as informações desejadas. Essa manipulação pode levar a diversas preocupações de segurança, como a geração de código malicioso, instruções ao agente para fornecer informações incorretas ou a revelação de dados confidenciais.

Embora a tecnologia de IA tenha facilitado muitas tarefas, ela também apresenta riscos quando cai em mãos erradas. Indivíduos com intenções maliciosas, como fraudadores e atacantes cibernéticos, podem explorar essa tecnologia para criar conteúdo prejudicial. Especificamente no caso de LLMs, há várias ameaças potenciais a serem observadas: descoberta de vulnerabilidades e geração de exploits; ataques automatizados de fraude ou golpes; ataques de engenharia social personalizados; código malicioso gerado por ferramentas de geração de código; e fácil acesso a conteúdo para planejar ataques ou incitar violência. É importante estar vigilante e implementar medidas de segurança para mitigar esses riscos associados a LLMs e prevenir o uso indevido da tecnologia.

A propriedade de conteúdo gerado por IA e as implicações de direitos autorais dos dados de treinamento usados em modelos de IA permanecem temas controversos, gerando inúmeras discussões. Ao utilizar modelos de IA generativa para fins comerciais, é crucial considerar cuidadosamente os aspectos de propriedade intelectual. Aqui estão duas considerações: os modelos de IA generativa podem ser treinados com dados proprietários e protegidos por direitos autorais; assim como outros softwares, esses modelos estão sujeitos a licenças que ditam seu uso. Para proteger os direitos de propriedade intelectual, garanta o uso adequado do conteúdo gerado, estabelecendo acordos legais e aderindo aos termos e condições das licenças que regem os modelos de IA.

As considerações legais se estendem a novas e emergentes tecnologias, como a IA generativa, e as leis existentes continuam aplicáveis. Processos de tomada de decisão automatizada devem estar atentos a vieses e discriminação para evitar ações regulatórias ou disputas legais para desenvolvedores ou implantadores. Além disso, deve-se ter cuidado ao fazer alegações sobre a funcionalidade ou os resultados de um modelo ou algoritmo. Há também a necessidade de abordar a responsabilidade por produtos, o que pode levar a litígios. A regulamentação hoje é influenciada por leis existentes e por propostas recentes, destinadas a abordar seu impacto. Algumas regulamentações notáveis propostas incluem: o Ato de IA da UE, que propõe um framework regulatório abrangente para governar o desenvolvimento, implantação e uso de sistemas de IA na União Europeia; o Ato de Responsabilidade Algorítmica dos EUA, que visa estabelecer transparência, justiça e responsabilidade em sistemas de decisão automatizada, incluindo os que utilizam IA; a abordagem de regulamentação de IA do Japão de 2023; e as ações de IA responsável da administração Biden-Harris de 2023, que introduziram iniciativas para promover o desenvolvimento e a adoção responsáveis de IA em diversos setores. Além disso, existem regulamentações em nível estadual, como a regulamentação da Califórnia sobre ferramentas de decisão automatizada, que foca na regulação de sistemas de decisão automatizada.

## Considerações éticas

Nesta seção, vamos explorar as considerações éticas relacionadas à IA generativa, examinando potenciais riscos e desafios. Discutiremos a presença de vieses em dados e sistemas de IA, bem como a confiabilidade dos modelos gerados. Além disso, abordaremos ações essenciais que podem ser tomadas para lidar com essas preocupações éticas. Como primeira parte deste módulo, falaremos sobre justiça e viés nos dados. É importante entender que o tamanho dos dados e a qualidade dos dados são coisas distintas. Ter grandes quantidades de dados não significa necessariamente ter dados de boa qualidade. Ao considerar justiça e viés nos dados, existem dois tipos principais de viés: viés humano nos dados e viés humano anotado.

Os vieses humanos nos dados estão relacionados a percepções sociais, estereótipos e fatores históricos. Esses vieses surgem de noções pré-concebidas, influências culturais e experiências passadas. Por exemplo, em um estudo, frases geradas pelo GPT-3 foram analisadas quanto ao sentimento, revelando diferenças raciais nos escores de sentimento. Embora existam muitos vieses humanos nos dados, não os cobriremos todos nesta discussão, mas aqui estão alguns exemplos: viés estereotipado refere-se a generalizações pré-concebidas sobre certos grupos; injustiça histórica diz respeito a vieses resultantes de práticas discriminatórias ou desigualdades passadas; associações implícitas envolvem vieses formados inconscientemente devido a influências culturais ou sociais.

O segundo tipo de viés é o viés humano anotado na coleta e anotação de dados. Esse viés é particularmente relevante no contexto da IA generativa, pois os modelos de IA generativa são treinados ou ajustados usando dados anotados por humanos. Esse tipo de viés reflete erros ou limitações no julgamento e raciocínio humano. Há também críticas sobre as qualificações dos anotadores humanos e como eles são selecionados para essas tarefas. Aqui estão alguns exemplos desse tipo de viés: erro de amostragem ocorre quando a amostra de dados usada para anotação não representa toda a população, levando a resultados enviesados ou imprecisos; viés de confirmação envolve a tendência de favorecer informações que apoiam crenças existentes, ignorando evidências contrárias; e falácia anedótica surge quando anedotas ou experiências pessoais recebem mais peso do que dados objetivos ou evidências estatísticas.

Esses tipos de vieses criam um ciclo de reforço de viés na IA generativa. Nesse ciclo, o viés nos dados de entrada gera viés no modelo de IA, que, por sua vez, produz resultados enviesados. A gravidade desses resultados pode variar de tóxicos a discriminatórios. Outro aspecto significativo é a alucinação do modelo, que discutiremos na próxima seção. Na fase final desse ciclo, as pessoas aprendem e usam dados enviesados, reforçando ainda mais os vieses existentes. Esse ciclo de feedback continua à medida que mais dados enviesados são gerados e usados como entrada novamente. Note que o termo "ciclo de reforço de viés" refere-se ao processo cíclico em que os vieses nos dados de entrada influenciam os vieses no modelo de IA, e os dados de saída enviesados são então usados para reforçar os vieses existentes.

Além de abordar o viés nos dados, é crucial considerar a confiabilidade e a precisão dos modelos de IA. Um aspecto significativo a discutir é o fenômeno da alucinação. A alucinação refere-se à geração de saídas pelo modelo que podem parecer plausíveis, mas que são, na verdade, imprecisas ou sem sentido devido a limitações de compreensão. Embora inicialmente pareça uma fraqueza menor, isso pode representar um risco significativo à medida que os modelos de IA se tornam mais convincentes e as pessoas passam a depender mais deles. Essa dependência de saídas imprecisas pode resultar em uma degradação da qualidade da informação e em consequências potenciais. Assim, como consumidores de IA, precisamos ser cautelosos com as limitações de confiabilidade e precisão desses modelos, usando-os de forma consciente.

Sem entrar em detalhes técnicos complexos, vamos explorar os dois tipos de alucinação de modelo: intrínseca e extrínseca. No caso de alucinação intrínseca, o modelo produz uma saída que contradiz diretamente as informações fornecidas nos dados de origem. Por outro lado, na alucinação extrínseca, o modelo gera uma saída que não pode ser confirmada ou corroborada com base nos dados de origem disponíveis. Esses tipos de alucinação destacam os desafios de garantir saídas precisas e confiáveis de modelos de IA.

Outro aspecto crucial para garantir confiabilidade e precisão em sistemas de IA é abordar o viés algorítmico nos modelos. Modelos de IA generativa têm o potencial de gerar resultados enviesados ou estereotipados. Por exemplo, um estudo revelou que histórias geradas pelo GPT-3 exibiram numerosos estereótipos de gênero. Vários fatores contribuem para o viés algorítmico, incluindo a falta de transparência nos dados de entrada, a dificuldade de rastrear os dados originais e o limitado processo de verificação de fatos. Esses fatores sublinham a importância de abordar e mitigar ativamente o viés nos modelos de IA para promover justiça e precisão em suas saídas.

Para mitigar os riscos e limitações associados à IA, é essencial adotar uma abordagem abrangente. Neste módulo, discutimos várias questões éticas relacionadas a dados, como viés nos dados, modelos tóxicos, desinformação, alucinação de modelos e usuários maliciosos. Lidar com esses desafios requer uma frente unida. Para combater o viés nos dados, é crucial examinar fatias de dados e considerar atualizações mais frequentes. Abordar modelos tóxicos exige uma abordagem multifacetada, que inclui avaliar a qualidade dos dados, implementar ferramentas de pós-processamento e estabelecer barreiras em torno de grandes modelos de linguagem. Curar dados adicionais para ajustes também pode ajudar a mitigar esses riscos. Ao lidar com desinformação ou riscos de informação, é importante scrutinizar a fonte das informações, o que pode envolver curar dados para ajustes ou treinar seu próprio modelo para garantir precisão e confiabilidade.

Para identificar e lidar com usuários maliciosos, medidas regulatórias são necessárias para detectar esses maus atores. Implementar regulamentações apropriadas pode ajudar a proteger contra comportamentos maliciosos e manter a integridade dos sistemas de IA. Regular o uso de grandes modelos de linguagem é necessário para abordar riscos e limitações com a crescente adoção da IA generativa; as regulamentações precisam abranger todos os potenciais riscos e garantir práticas responsáveis. Auditar modelos de IA generativa é crucial para mitigar os riscos e desafios discutidos. Este material propõe uma abordagem de três camadas para regulamentação e auditoria: primeiro, auditorias de governança, que se concentram em provedores de tecnologia, especialmente grandes empresas que fornecem os modelos; a segunda camada envolve a auditoria dos próprios modelos; a terceira camada refere-se à auditoria no nível da aplicação, avaliando os riscos com base no uso pelos usuários. Para facilitar a governança de dados e modelos por meio de auditorias, o catálogo da comunidade Databricks fornece ferramentas para gerenciar e controlar o acesso aos seus dados e recursos de IA.

## Interação entre humanos e IA


